
# Chapter 5-3

[혼공머5_3.ipynb](%ED%98%BC%EA%B3%B5%EB%A8%B85_3.ipynb)

### 정형데이터

- 특정 구조로 되어있는 데이터
- csv, 데이터베이스, 엑셀에 저장하기 쉬움


### 비정형데이터

- 텍스트데이터, 사진데이터, 디지털 음악 데이터와 같이 특정 구조를 가지지 않는 데이터

### 앙상블 학습

- 앙상블 학습은 더 나은 예측을 생성하기 위해 두 개 이상의 학습자(예: 회귀 모델, 신경망)를 집계하는 머신 러닝 기술 → 정형데이터를 다루는데 성과가 좋음

### 랜덤 포레스트

- 대표적인 결정트리 기반의 앙상블 학습방법
- 부트 스트랩 샘플을 사용하여 랜덤하게 일부 특성을 선택하여 트리를 만듦
- bagging을 할때 강한 예측 변수가 있으면 만들어지는 트리들의 구조가 비슷해지기 때문에 트리를 de-colerate하기 위해 사용

### 부트스트랩 (boot strap)

- 샘플에서 복원 추출을 통해 만들어지는 샘플들로 훈련세트의 크기와 같게 만든다.

### OOB(Out Of Bag) 샘플

- 부트스트랩 샘플은 복원 추출을 통해 만들어지기 때문에 특정 샘플은 선정되지 않을 수도 있음 → 이때 한번도 선정되지 않은 샘플을 사용하여 test를 할 수 있다(validation set으로 사용)

### 엑스트라 트리

- 랜덤포레스트와 비슷하게 결정트리를 사용하여 앙상블 모델을 만들지만 부트스트랩을 사용하지 않는다. → 결정트리를 만들때 전체 훈련세트 사용
- 랜덤하게 노드를 분할해 과대 적합을 감소시킨다.

### 부스팅(Boosting)

- 얕은 classifier를 순차적으로 사용하여 만든 결정트리를 통해 학습시키는 방법
- residual을 계산하여 다음 모델은 그것을 통해 학습하여 최정적으로는 가중합을 사용한다.

### 그레디언트 부스팅 (Gradient Boosting)

- 깊이가 얕은 결정트리를 사용하여 이전 트리의 오차를 보완하는 방식의 앙상블 기법
- 깊이가 얕은 트리를 사용하기 때문에 과대적합에 강하지만 경사하강법을 통해 과대적합을 방지한다.

### 히스토그램 기반 그레디언트 부스팅 (Histogram-based Gradiant Boosting)

- 입력특성을 256개의 구간으로 나누어 노드를 분할할때 최적의 분할을 빠르게 찾게 만든 부스팅 기법
- 누락된 특성이 있어도 따로 전처리를 할 필요가 없음
- 대표적인 라이브러리: XGBoost, LightGBM
