
# 4주차 과제

```
dt = DecisionTreeClassifier(random_state=42)
dt.fit(sub_input, sub_target)
print(dt.score(val_input, val_target)) # 0.864423076923077

# 2. 첫 번째 '수정' (max_depth=10)
dt = DecisionTreeClassifier(random_state=42, max_depth=10) # 
dt.fit(sub_input, sub_target)
print(dt.score(val_input, val_target)) # 0.8528846153846154

# 3. 두 번째 '수정' (max_depth=12)
dt = DecisionTreeClassifier(random_state=42, max_depth=12) # 
dt.fit(sub_input, sub_target)
print(dt.score(val_input, val_target)) # 0.8596153846153847
```

- 위와 같이 직접 파라미터 값을 수정하며 최적의 파라미터를 찾기에는 너무 방대한 양의 값들을 직접 넣고 실행하며 비교해야한다.
- 이를 해결 하기 위해 Chapter 5-2에 있는 그리드 서치나 랜덤서치를 통해 파라미터의 범위를 지정해주어 최적의 하이퍼파라미터를 자동으로 탐색해주는 방법을 사용한다.

### 하이퍼파라미터 범위 지정

```
params = {'min_impurity_decrease': np.arange(0.0001, 0.001, 0.0001), #0.0001~0.001까지 0.0001씩 더한 원소가 9개인 배열
          'max_depth': range(5, 20, 1), #정수만
          'min_samples_split': range(2, 100, 10) #정수만 
          }
```

### 파인튜닝 진행

```python
gs = GridSearchCV(DecisionTreeClassifier(random_state=42), params, n_jobs=-1)
```

### 최적의 파라미터를 사용한 모델의 성능확인

```
dt = gs.best_estimator_

print(dt.score(test_input, test_target)) 
```
