# 8주차 과제

1. **RNN (Recurrent Neural Network)**

→ 문제점: 긴 시퀀스에서 **장기 의존성 학습 불가(Gradient Vanishing/Exploding)**

→ 보완 아이디어: 이전 정보 저장 구조 필요

1. **LSTM (Long Short-Term Memory)**

→ 문제점: RNN의 Gradient Vanishing

→ 보완 아이디어: 게이트 구조(입력/망각/출력 게이트)로 보완하여 중요한 기억을 오래 유지

1. **Attention**

→ 문제점: LSTM의 긴 문맥 제한, 정보 압축 손실

→ 보완 아이디어: 필요한 정보에만 집중(가중치) 하여 멀리 있는 단어도 직접 참조

1. **Transformer**

→ 문제점: RNN/LSTM의 순차 처리, 느린 속도

→ 보완 아이디어: 완전한 Attention 기반 + 병렬 연산 구조

1. **BERT (Bidirectional Encoder Representations from Transformers)**

→ 문제점: 단방향 문맥 모델의 의미 이해 부족

→ 보완 아이디어: Transformer의 Encoder만 사용하여 문장을 양방향으로 동시에 이해