# 딥러닝 모델(RNN, LSTM, Attention, BERT, Transformer)의 개념 공통점과 차이점 한줄 정리 (문제점-보완 위주로) 

## 딥러닝 모델 공통 및 차이점 

모든 모델의 공통점은 시계열 데이터(Sequential Data, 텍스트나 음성 등 순서가 있는 데이터)
를 처리하여 문맥과 패턴을 학습한다는 점입니다.<br>

| 모델 | 개념 | 차이점 (문제점 $\rightarrow$ 보완) |
| :--- | :--- | :--- |
| **RNN** | 순환 구조를 통해 이전 단계의 정보를 현재 단계로 전달하는 신경망 | 시계열 데이터 처리는 가능하나, 문장이 길어질수록 앞부분의 정보를 잊어버리는 문제가 발생함. |
| **LSTM** | RNN에 셀 상태와 게이트 구조를 추가한 모델 | RNN의 장기 의존성 문제를 게이트(삭제/입력/출력)로 해결했지만, 여전히 순차적으로 계산해야 하므로 병렬 처리가 불가능하여 학습 속도가 느림. |
| **Attention** | 출력 시점에 입력 시퀀스의 전체 정보를 다시 참고하되, 중요한 부분에 집중하는 메커니즘 | Seq2Seq(RNN 기반)의 고정 길이 문맥 벡터로 인한 정보 손실 문제를 해결하기 위해, 디코더가 출력할 때마다 중요한 부분에 가중치를 두어 참조함. |
| **Transformer**| RNN이나 CNN 없이 오직 어텐션 메커니즘만으로 구성된 아키텍처 | RNN의 순차적 처리로 인한 느린 속도 문제를 해결하기 위해, 재귀를 없애고 Self-Attention을 도입하여 데이터를 병렬로 처리할 수 있게 함. |
| **BERT** | Transformer의 인코더 부분만을 활용한 양방향 사전 학습 모델 | 기존 언어 모델의 단방향성(이전 단어만 봄) 한계를 극복하기 위해, Masked LM 방식을 통해 문맥을 양방향으로 깊이 있게 이해함. |

