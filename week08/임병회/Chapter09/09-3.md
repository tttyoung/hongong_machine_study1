# LSTM과 GRU 셀

## LSTM 구조

Long Short-Term Memory 약자<br>



셀 상태 - LSTM 셀 내부에서 순환만 되는 값<br>

<img width="416" height="293" alt="image" src="https://github.com/user-attachments/assets/999459cb-91d1-4043-ae12-972cd6ecf9a8" />

## LSTM 신경망 훈련하기

```python
# LSTM
from tensorflow import keras
model=keras.Sequential()
model.add(keras.layers.Embedding(500,16,input_length=100))
model.add(keras.layers.LSTM(8))
model.add(keras.layers.Dense(1,activation='sigmoid'))
# 컴파일, 훈련
rmsprop=keras.optimizers.RMSprop(learning_rate=1e-4)
model.compile(optimizer=rmsprop,loss='binary_crossentropy',metrics=['accuracy'])
checkpoint_cb=keras.callbacks.ModelCheckpoint('best_lstm-model.h5',
                                              save_best_only=True)
early_stopping_cb=keras.callbacks.EarlyStopping(patience-3,
                                                restore_best_weights=True)
history=model.fit(train_seq,train_target, epochs=100, batch_size=64,
                  validation_data=(val_seq, val_target),
                  callbacks=[checkpoint_cb, early_stopping_cb])
# 손실 그래프
import matplotlib.pyplot as plt
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.show()
```

<table>
  <tr>
    <td align="center">
      <img src="https://github.com/user-attachments/assets/92e06356-4eff-45a3-b3c1-714e8bde9c27" width="100%">
    </td>
    <td align="center">
      <img src="https://github.com/user-attachments/assets/ef17973c-4b13-40b0-932e-c1a0c8f84b46" width="100%">
    </td>
  </tr>
  <tr>
    <td align="center"><b>순환 신경망 사용 시</b></td>
    <td align="center"><b>LSTM 사용 시</b></td>
  </tr>
</table>

## 드롭아웃 in 순환층

```python
# 30퍼 드롭아웃
model12=keras.Sequential()
model12.add(keras.layers.Embedding(500,16,input_length=100))
model12.add(keras.layers.LSTM(8, dropout=0.3))
model12.add(keras.layers.Dense(1,activation='sigmoid'))
# 훈련
rmsprop=keras.optimizers.RMSprop(learning_rate=1e-4)
model2.compile(optimizer=rmsprop, loss='binary_crossentropy',
                metrics=['accuracy'])
checkpoint_cb=keras.callbacks.ModelCheckpoint('best-dropout-model.h5',
                                              save_best_only=True)
early_stopping_cb=keras.callbacks.EarlyStopping(patience=3,
                                                restore_best_weights=True)
history=model2.fit(train_seq, train_target, epochs=100, batch_size=64,
                   validation_data=(val_seq, val_target),
                   callbacks=[checkpoint_cb, early_stopping_cb])
# 손실 그래프
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.show()
```

<img width="507" height="295" alt="image" src="https://github.com/user-attachments/assets/ef90d887-a318-4ef6-9697-406da25ae32c" />

두 손실 간 차이 줄어듦<br>

## 2개의 층 연결

<img width="508" height="212" alt="image" src="https://github.com/user-attachments/assets/05499180-a444-4efc-aaf8-b8a41f6ae523" />

```python
# 마지막 제외 모든 은닉상태 출력
model3=keras.Sequential()
model3.add(keras.layers.Embedding(500,16,input_length=100))
model3.add(keras.layers.LSTM(8, dropout=0.3, return_sequences=True))
model3.add(keras.layers.LSTM(8, dropout=0.3))
model3.add(keras.layers.Dense(1,activation='sigmoid'))
# 훈련
rmsprop=keras.optimizers.RMSprop(learning_rate=1e-4)
model3.compile(optimizer=rmsprop, loss='binary_crossentropy',
                metrics=['accuracy'])
checkpoint_cb=keras.callbacks.ModelCheckpoint('best-2rnn-model.h5',
                                              save_best_only=True)
early_stopping_cb=keras.callbacks.EarlyStopping(patience=3,
                                                restore_best_weights=True)
history=model3.fit(train_seq, train_target, epochs=100, batch_size=64,
                   validation_data=(val_seq, val_target),
                   callbacks=[checkpoint_cb, early_stopping_cb])
# 손실 그래프
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['train','val'])
plt.show()
```

<img width="508" height="303" alt="image" src="https://github.com/user-attachments/assets/4ab275de-076e-4f87-9ead-c4a90cd44480" />

과대적합 제어하면서 손실 낮춘 거 확인<br>

## GRU 구조

Gated Recurrent Unit의 약자, LSTM을 간소화한 버전<br>

<img width="416" height="308" alt="image" src="https://github.com/user-attachments/assets/2e43a23e-1869-4132-9794-73dbb364e87f" />

LSTM : 입력, 삭제, 출력 to 셀 상태<br>
GRU : 삭제, 업데이트 후 바로 은닉상태<br>

## GRU 신경망 훈련하기

```python
#GRU
model4=keras.Sequential()
model4.add(keras.layers.Embedding(500,16,input_length=100))
model4.add(keras.layers.GRU(8))
model4.add(keras.layers.Dense(1,activation='sigmoid'))
# 훈련
rmsprop=keras.optimizers.RMSprop(learning_rate=1e-4)
model4.compile(optimizer=rmsprop, loss='binary_crossentropy',
                metrics=['accuracy'])
checkpoint_cb=keras.callbacks.ModelCheckpoint('best-gru-model.h5',
                                              save_best_only=True)
early_stopping_cb=keras.callbacks.EarlyStopping(patience=3,
                                                restore_best_weights=True)
history=model4.fit(train_seq, train_target, epochs=100, batch_size=64,
                   validation_data=(val_seq, val_target),
                   callbacks=[checkpoint_cb, early_stopping_cb])
# 손실 그래프
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['train', 'val'])
plt.show()
```

<img width="527" height="313" alt="image" src="https://github.com/user-attachments/assets/e0b9cb6b-3611-4b2b-bef9-b10d59e8b6a1" />

훈련 얼추 잘 됨<br>

