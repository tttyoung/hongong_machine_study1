# 6주차 과제 - 딥러닝 훈련 기술을 조사하고 각각의 역할과 사용법을 정리(배치 정규화, 학습률 스케줄링, 데이터 증강, 가중치 초기화 등..)

### 배치 정규화 (Batch Normalization, BN)

- 학습 과정에서 각 레이어에 들어가는 입력 데이터의 분포가 계속 바뀌는 현상(Internal Covariate Shift)을 막기 위해, 데이터를 평균 0, 분산 1로 정규화하는 과정

역할

- 각 층 입력의 **분포 변화를 줄여(Internal Covariate Shift)** 학습을 안정화
- 과적합(overfitting)을 어느 정도 방지한다(미세한 regularization 효과)
- 초기화 의존도 감소: 가중치 초기화에 덜 민감해진다.

사용법

- Conv2d와 ReLU사이에 배치한다.

```python
    x = layers.Conv2D(32, (3, 3), padding="same")(inputs)
    x = layers.BatchNormalization()(x)   # 배치 정규화
    x = layers.ReLU()(x)
```

### 학습스케줄

- 학습이 진행됨에 따라 가중치를 업데이트하는 보폭인 학습률(Learning Rate)을 조절하는 기술

역할

- 최적해 수렴: 학습 초기에는 큰 보폭으로 빠르게 이동하고, 학습 후반에는 작은 보폭으로 미세하게 조정하여 최적점(Global Minimum)에 정확히 도달하게 합니다.
- Local Minima 탈출: 학습 중간에 학습률을 다시 높이는 방식(예: Cosine Annealing)을 써서 얕은 함정에 빠지는 것을 방지하기도 합니다.

사용법

```python
lr_schedule = keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate=initial_lr,
    decay_steps=1000,       # 몇 step마다 감소시킬지
    decay_rate=0.96,        # 감소 비율
    staircase=True          # True면 계단형 감소
)
```

### 데이터 증강

- 갖고 있는 데이터셋에 인위적인 변화를 주어 데이터의 양을 늘리고 다양성을 확보하는 기술

역할

- 원래 있는 데이터에서 **여러 변형된 데이터를 인위적으로 생성**하여 데이터 수를 늘리는 효과
- 모델이 다양한 상황에 대해 더 잘 일반화할 수 있게 함 → 과대적합 줄이기

사용법

```python
# 데이터 증강 레이어 블록 정의
data_augmentation = tf.keras.Sequential([
    layers.RandomFlip("horizontal"),   # 좌우 반전
    layers.RandomRotation(0.1),        # 10% 범위 내 회전
    layers.RandomZoom(0.1),            # 10% 범위 내 확대/축소
])

# 모델에 포함시키기
input_shape = (32, 32, 3)
model = tf.keras.Sequential([
    layers.Input(shape=input_shape),
    data_augmentation,  # [핵심] 입력 데이터가 들어오면 여기서 변형됨
    layers.Conv2D(32, (3, 3), activation='relu'),
    # ... 이후 레이어들
])
```

### 가중치 초기화

- 모델을 정의(Build)하는 시점에 적용
- PyTorch는 기본적으로 괜찮은 초기화 값을 제공하지만, 특정 활성화 함수(ReLU 등)에 맞춰 **He Initialization** 등을 명시적으로 적용할 때 사용

역할

- 학습 초기 단계에서 gradient가 너무 커지거나 너무 작아지는 것을 방지
- 적절한 초기값은 학습 속도와 최종 성능에 큰 영향을 끼친다.

사용법

```python
he_dense = layers.Dense(
    128,
    activation="relu",
    kernel_initializer="he_normal"   # He 초기화
)

xavier_dense = layers.Dense(
    64,
    activation="tanh",
    kernel_initializer="glorot_uniform"  # Xavier 초기화
)
```
