# 딥러닝 훈련 기술을 조사하고 각각의 역할과 사용법을 정리하기. (배치 정규화, 학습률 스케줄링, 데이터 증강, 가중치 초기화 등..)

## 배치 정규화

### 역할
신경망이 깊어질수록, 각 층을 통과하면서 데이터의 분포가 계속 바뀝니다.마치 각 층마다 다른 언어로 말하는 것과 같아서, 다음 층이 학습하기 매우 어려워집니다.
배치 정규화는 활성화 함수를 통과하기 직전에 이 정규화를 적용하여,
데이터가 항상 활성화 함수의 중앙에 분포하도록 만들어 
딥러닝 모델의 학습을 안정화하고 속도를 향상시키기 위해 사용됩니다.<br>

### 사용법

Dense나 Conv2D 층과 활성화 함수 층 사이에 BatchNormalization 레이어를 추가합니다.

```python
model.add(keras.layers.Dense(100))
model.add(keras.layers.BatchNormalization()) # <-- 배치 정규화 층 추가
model.add(keras.layers.Activation('relu'))
```

## 학습률 스케줄링

### 역할

훈련 초기에는 최적점에서 멀리 떨어져 있으므로 
큰 학습률로 성큼성큼 빠르게 다가가는 것이 좋고, 
훈련 후기에는 최적점 근처에 도달했으므로
작은 학습률로 미세하게 조정해야 최적점을 지나치지 않고 안정적으로 안착할 수 있기에
학습률 스케쥴링은 훈련 과정 동안 학습률을 동적으로 조절하여
더 빠르고 안정적으로 최적점에 수렴하도록 돕습니다.<br>

### 사용법

옵티마이저를 생성할 때 schedule을 지정하거나,
LearningRateScheduler 콜백을 사용합니다.<br>

```python
# 1. 간단한 방법: 옵티마이저의 decay 인자 사용
optimizer = keras.optimizers.SGD(learning_rate=0.1, decay=1e-6)

# 2. 유연한 방법: 콜백 사용
lr_scheduler = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)
model.fit(..., callbacks=[lr_scheduler])
```
ReduceLROnPlateau 콜백:<br>
monitor로 지정한 지표가,
min_delta에서 설정한 최소 개선량 이상으로 좋아지지 않는 현상이
patience에서 설정한 에포크 횟수 동안 연속으로 발생할 때 학습률 줄임<br>

## 데이터 증강

### 역할

실제 세계에서는 데이터를 수집하는 데 한계가 있습니다.
데이터가 부족하면 모델이 훈련 데이터를 '암기'해버리는 과대적합이 발생하기 쉽습니다.
데이터 증강은 더 많은, 그리고 더 다양한 훈련 데이터를 인공적으로 만들어 
모델의 일반화 성능을 높이고 과대적합을 방지합니다.<br>

### 사용법

ImageDataGenerator나 tf.keras.layers.RandomFlip,
RandomRotation 등 전처리 레이어를 사용합니다.<br>

```python
data_augmentation = keras.Sequential([
    keras.layers.RandomFlip("horizontal"),
    keras.layers.RandomRotation(0.1),
])

model = keras.Sequential([
    data_augmentation, # 모델의 첫 부분에 추가
    # ... (나머지 모델 층)
])
```

## 가중치 초기화

### 역할

적절한 분산을 가진 작은 난수로 가중치를 초기화하여, 
신호가 여러 층을 통과하더라도 안정적으로 유지되도록 합니다.<br>
He 초기화 (He Initialization):
ReLU 활성화 함수와 함께 사용할 때 가장 좋은 성능을 보인다고
알려진 표준 초기화 방법입니다. ReLU는 음수 0처리하므로 평균이 0이고, 
표준편차가 **sqrt(2 / fan_in)**인 정규분포에서 가중치를 샘플링합니다.<br>
Xavier/Glorot 초기화 (Xavier/Glorot Initialization):
Sigmoid나 tanh 활성화 함수와 잘 작동합니다.
평균이 0이고, 표준편차가 **sqrt(2 / (fan_in + fan_out))**인 정규분포에서
가중치를 무작위로 샘플링합니다.<br>

### 사용법

Dense나 Conv2D 층(이미지 특징을 추출하는 신경망 층)의 
kernel_initializer 인자를 설정합니다.
케라스는 기본적으로 활성화 함수에 맞는 좋은 초기화 방법을 자동으로 사용합니다.


```python
# He 초기화를 명시적으로 사용
model.add(keras.layers.Dense(100, activation='relu', 
                            kernel_initializer='he_normal'))
```
