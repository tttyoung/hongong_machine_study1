# 07-2 | 심층 신경망

성능을 높일려면 여러 층을 추가하면됨

![image.png](image.png)

**은닉층** : 입력층과 출력층 사이에 있는 모든 층

**은닉층의 활성화 함수** : 모델을 비선형적으로 만들어 복잡한 패턴 학습을 가능하게 해줌.

- 코드
    
    ```python
    # 입력층
    inputs = keras.layers.Input(shape=(784,))
    # 은닉층. 100개의 뉴런을 가지고 활성화 함수로 시그모이드 함수를 사용함.
    dense1 = keras.layers.Dense(100, activation='sigmoid', input_shape=(784,))
    # 출력층
    dense2 = keras.layers.Dense(10, activation='softmax')
    ```
    

### 심층 신경망 만들기

Sequential 클래스에 앞서 만든 inputs, dense객체들 추가시키면 심층 신경망(DNN)이 됨.

- 코드
    
    ```
    # 심층 신경망 만들기
    model = keras.Sequential([inputs, dense1, dense2])
    ```
    

인공 신경망의 높은 성능은 이렇게 층을 여러개 추가해 연속적인 학습을 진행하는 능력에서 나옴.

### ReLU 함수

기존의 시그모이드 함수

![image.png](image%201.png)

→ 오른쪽과 왼쪽 끝으로 갈수록 그래프가 누워있기 때문에 올바른 출력을 만드는데 신속하게 대응하지 못한다.

=

1) 그래프가 누워있음 = 기울기가 0에 가까워짐

2) 신경망은 역전파(Backpropagation)에서 기울기를 이용해 가중치를 업데이트하는데,

그래프가 누워서 기울기가 0에 가까워지면 가중치 업데이트가 안됨.

3) 학습이 매우 느려져 신속히 정답 방향으로 못 움직임.

즉, 출력이 틀려도 빠르게 수정하기 어려움

이를 개선하기위해

**ReLU 함수** 

: 입력이 양수→입력 그대로 출력, 입력이 음수→ 0 출력

![image.png](image%202.png)

시그모이드 함수 대신 렐루 함수를 적용하면 성능이 조금 향상된다는 것을 알 수 있음.

### 옵티마이저

신경망에는 하이퍼파라미터가 많음. 

다양한 종류의 경사 하강법 알고리즘을 제공하는 옵티마이저 또한 그 중 하나

![image.png](image%203.png)

1. 기본적인 SGD(확률적 경사 하강법) (이름만 SGD이고 미니배치를 사용함)
2. momentum → 모멘텀 매개변수를 0보다 큰 값으로 지정해 사용
    
    경사하강법으로 이동할 때 관성을 부여해 최적화하는 기법. 이전 기울기의 크기를 고려하여 추가적으로 이동해, Local minimun에서 빠져나갈 수 있게함. 
    
3. Nesterov momentum (네스테로프 모멘텀) → nesterov 매개변수를 True로 바꿔 사용
    
    : 모멘텀을 2번 반복해 구현
    

최적점에 가까이 갈수록 학습률을 낮추는 ‘적응적 학습률’을 사용하는 옵티마이저

1. Adagrad
    
    특성별로 학습률을 다르게 조절하는 기법.
    
2. RMSprop
    
    특성별로 학습률을 다르게 조절하되, 최근 step의 기울기를 많이 반영하고 먼 과거의 step 기울기는 조금만 반영함.
    
3. Adam
    
    Momentum + RMSProp 의 장점을 결합한 알고리즘