# 6주차 과제

1. **배치 정규화 (Batch Normalization)**
    1. 한 layer의 입력을 **정규화하여 학습을 안정화**하는 기법
    2. 역할
        1. 학습 **속도** 향상
        2. **gradient explode / diminish 방지**
        3. 초기값과 학습률에 **덜 민감**
    3. 사용법
        1. python
            
            ```python
            nn.BatchNorm2d(num_features) # CNN : 2D, MLP에서는 BatchNorm1d 사용
            ```
            

1. **학습률 스케줄링 (Learning Rate Scheduling)**
    1. 학습이 진행됨에 따라 **learning rate를 점진적으로 조절**하는 기법
    2. 역할
        1. **초반에는 빠르게** 학습, **후반에는 안정적** 수렴
        2. **오버슈팅 방지**
    3. 사용법
        1. python
            
            ```python
            torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1) # 10 epoch마다 LR을 0.1배로 감소
            ```
            

1. **데이터 증강 (Data Augmentation)**
    1. 기존 학습 데이터를 회전, 뒤집기 등 **변형하여 학습 데이터를 늘리는** 방법
    2. 역할
        1. **overfitting 방지**
        2. **일반화 성능** 향상
    3. 사용법
        1. python
            
            ```python
            transform = transforms.Compose([transforms.RandomHorizontalFlip(), transforms.RandomRotation(15), transforms.ColorJitter(brightness=0.2)])
            ```
            

1. **가중치 초기화 (Weight Initialization)**
    1. 학습 시작 시 신경망의 **가중치를 적절한 범위로 설정**하는 방법
    2. 역할
        1. **gradient explode / diminish 방지**
        2. **빠르고 안정적**인 수렴
    3. 사용법
        1. python
            
            ```python
            nn.init.xavier_uniform_(layer.weight) # Xavier(Glorot) 초기화
            ```
            

1. **정규화 (Regularization, L1/L2)**
    1. 가중치 크기에 **패널티를 부여하는 손실** 항 추가
    2. 역할
        1. **복잡한 모델 억제**
        2. **과적합 감소**
    3. 사용법
        1. python
            
            ```python
            optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4) # weight_decay = L2 규제
            ```