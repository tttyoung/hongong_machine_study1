# 과제:
DNN 모델과 이번 주에 만든 CNN 모델의 성능과 훈련 속도를 비교하고,
왜 차이가 발생하는지 분석하기. 
(Optional) CNN의 한계 분석 및,
'Deep Residual Learning for Image Recognition' (ResNet) 논문 리뷰하고 
핵심 아이디어(Residual Block) 정리하기. 

## DNN 모델과 CNN 모델의 성능 및 훈련 속도 비교 분석

1. 성능 비교 분석
  DNN은 이미지를 1차원 벡터로 펼쳐야하여 이 때
이미지를 2차원 그대로 받는 CNN과 달리 픽셀 간 공간적 관계 정보가 손실되어 CNN의 성능이
더 좋게 나올 것이다.<br>

2. 훈련 속도 비교 분석
      에포크당 훈련 시간은 모델의 복잡도와 파라미터 수에 따라 달라지므로
    직접 측정해봐야 합니다.
    일반적으로 CNN이 더 많은 연산(합성곱)을 수행하므로 DNN보다 약간 더 느릴 수 있습니다.<br>

## CNN의 한계 및 ResNet 논문 리뷰

1. CNN의 한계 분석 (깊이에 따른 문제)
  문제점1: 그래디언트 소실/폭발<br>
    층을 통과하는 과정에서 그래디언트가 너무 작아지거나 커지는 문제 발생<br>
  문제점2: 성능 저하 문제<br>
    일정 수준 이상으로 층이 깊어지면, 그래디언트 소실 문제 등으로 인해 훈련이 어려워져
    오히려 훈련 에러와 테스트 에러가 모두 증가하는 성능 저하 현상이 발생<br>

2.'Deep Residual Learning for Image Recognition' (ResNet) 논문 핵심 아이디어<br>
  층이 H(x)를 직접 학습하는 대신,입력과 출력의 차이(잔차, Residual)인 
  F(x) = H(x) - x를 학습하도록 구조를 바꿉니다. <br>
  구현 방법: 스킵 연결 (Skip Connection)
    (1)입력 x를 몇 개의 층을 통과시켜 F(x)를 계산합니다.
    (2)동시에, 입력 x를 아무런 변환 없이 그대로 건너뛰어 F(x)의 결과에 더해줍니다.
    (3)이 '건너뛰는 길'이 바로 스킵 연결 또는 아이덴티티 매핑(Identity Mapping)입니다.
이를 통해 그래디언트 소실/폭발 문제를 해결합니다.
