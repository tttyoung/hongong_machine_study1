# 3주차 과제

1. **Momentum**
    1. 기본 gradient descent는 진동 심함, 수렴 속도 느림, 최적점 지나침
    2. gradient descent의 이동 경로에 inertia(관성) 개념 도입 → 이전 단계의 이동 방향을 일정 비율로 유지하면서 새로운 방향으로 조금씩 수정
        1. 현재 기울기 + 이전 단계의 이동량
        2. local minimum을 해결할 수 있음
        3. 수식
            ![v : 속도, alpha : 모멘텀 계수](https://github.com/user-attachments/assets/2f42c800-33e3-425b-b11b-e9a224a49737)
            
            v : 속도, alpha : 모멘텀 계수
            

1. **AdaGrad (Adaptive Gradient)**
    1. 각 파라미터마다 학습률을 다르게 조정
        1. 모든 파라미터에 같은 학습률을 적용하면 비효율적
        2. 자주 업데이트되는 파라미터는 학습률 작게, 드물게 업데이트되는 파라미터는 학습률 크게
    2. 각 파라미터별로 기울기 제곱의 누적합 저장, 이를 이용해 학습률 조정
        1. but 누적합이 계속 커져서 학습률이 점점 0에 가까워지면 → 학습 멈춤
        2. 수식
            
            ![image.png](https://github.com/user-attachments/assets/b5f833aa-0a33-4431-af7d-555b44df328e)

1. **RMSProp (Root Mean Square Propagation)**
    1. AdaGrad 단점 개선 → 최근의 기울기만 고려하도록 만듦
    2. 지수 이동 평균 (Exponential Moving Average)로 계산
        1. 수식
            
            ![image.png](https://github.com/user-attachments/assets/fecd9044-5a68-41b2-aec8-cc32554a00b6)
            

1. **Adam (Adaptive Moment Estimation)**
    1. Momentum + RMSProp
        1. Momentum : 방향 부드럽게 조정
        2. RMSProp : 학습률 자동 조정
    2. 1차 모멘트(기울기의 평균), 2차 모멘트(기울기의 제곱 평균) 둘 다 추적 + bias correction 적용
        
        ![image.png](https://github.com/user-attachments/assets/ba620236-f874-493f-b17f-f7a61d26a2bd)

    3. L2 regularization 추가된 loss func를 Adam을 이용해서 최적화 할 경우 일반화 효과 떨어짐

1. **AdamW**
    1. Adam에서 Weight Decay(가중치 감소) 올바르게 개선
    2. weight decay를 기울기 업데이트와 분리해서 적용 → 파라미터 업데이트 시 별도로 decay시켜 과적합 방지
