# 04-1 | 로지스틱 회귀

다중 분류 : 타깃 데이터에 2개 이상의 데이터가 포함된 문제

### **로지스틱 회귀**

: 분류 모델, 선형 방정식 학습. 

이진분류에서는 시그모이드 함수를 이용해 z가 큰 음수일 때 0이 되고 큰 양수일 때 1이 되게 하여 0~1 사이 값이 되도록 함.

### 로지스틱 회귀로 다중 분류 수행

LogisticRegression클래스는 반복적인 최적화 알고리즘을 사용한다. max_iter매개변수로 반복 횟수를 지정할 수 있다.

또한, 계수의 제곱을 규제하며 매개변수는 C(1/람다)이다. C는 L2규제의 강도를 조절하며 C가 작을 수록 규제가 커진다. 

다중 분류에서는 소프트맥스 함수를 사용하여 z값을 확률로 변환한다.

**소프트맥스** 함수 : 여러 개의 선형 방정식 출력값을 0~1사이의 확률로 압축하고 전체 합이 1이 되게 한다. 일반적인 맥스 함수는 최댓값 중 하나를 고르지만 소프트맥스는 최댓값에 높은 확률을 할당하면서 다른값에도 작은 확률을 부여한다.

---

궁금한 점

Q . p.198- LogisticRegression클래스는 반복적인 알고리즘 사용? 반복 횟수 기본값 100?

A . 반복적인 알고리즘은 Gradient discent 같은 최적화 알고리즘을 말하는 것으로, 이는 손실을 줄이는 방향으로 가중치를 조금씩 수정하며 반복해 손실이 최소가 되는 시점을 찾는 최적화 알고리즘임..  종류는 newton-cg, lbfgs, liblinear, sag, saga 로 5가지가 있고 기본값은 lbfgs임.

1. newton-cg : 손실함수의 2차 미분정보를 사용해 곡률도 고려하여 최적화함. L2 규제 사용 가능
2. lbfgs : 2차 미분행렬을 직접 계산하지않고 근사해서 속도를 개선함. L2 규제 사용가능
3. liblinear : 경사하강법과 비슷하지만 한번에 하나의 파라미터만 업데이트한다는 차이점이 있다. L1,L2 규제 사용가능
4. sag : 경사하강법과 비슷하지만 이전 경사값을 현재 업데이트에 사용한다.  L2 규제 사용 가능
5. saga : sag의 업그레이드 버전으로 L1,L2,elasticent 규제 모두 사용가능한 방식

[https://coduking.tistory.com/entry/로지스틱-회귀-Solver-종류와-장단점](https://coduking.tistory.com/entry/%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1-%ED%9A%8C%EA%B7%80-Solver-%EC%A2%85%EB%A5%98%EC%99%80-%EC%9E%A5%EB%8B%A8%EC%A0%90)