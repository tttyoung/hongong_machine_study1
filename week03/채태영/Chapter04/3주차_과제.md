
# 3주차 과제

### 아래 5가지 옵티마이저의 개념을 설명하고, 어떤 문제를 해결하기 위해, 어떤 아이디어를 적용했는지 작성.
Momentu, Adagrad, RMSProp, Adam, AdamW

### 옵티마이저

- 딥러닝 모델을 학습시킬때, 손실함수의 값을 최소화 하기 위해 가중치(파라미터)를 업데이터 하는 과정을 최적화라고 하며, 이때 사용되는 알고리즘이 옵티마이저라고 한다.
- SGD에서는 현재 위치의 그래디언트만을 보고 가중치를 업데이트 한다. 그러나 특정지형(좁고 긴 골짜기)에서 비효율적인 진동하는 문제가 있기 때문에 이를 극복하기 위해 다양한 옵티마이저가 만들어짐.

### Momentum

- '관성(Momentum)의 개념을 도입한 옵티마이저.
- 공이 언덕을 굴러 내려갈 때 이전에 굴러오던 속도(관성)가 현재 속도에 영향을 주는 것과 같다.

<img width="602" height="402" alt="image" src="https://github.com/user-attachments/assets/291a9043-bb9a-49db-96fc-dfaa1abc9eaa" />


- 아래의 사진과 같은 local minimum에 빠져버리는 문제를 해결하기 위해 생겨난 기법으로 관성을 이용하여 global minimum에 가는 방향으로 유도하는 원리.

<img width="572" height="332" alt="image 1" src="https://github.com/user-attachments/assets/07e864a1-3682-4652-9b19-c31b424eb246" />


### Adagrad

- 각 가중치(파라미터)마다 맞춤형 학습률을 적용한다.
- Feature마다 중요도, 크기가 모두 다르기 때문에 각자 동일한 학습률을 적용하는것이 비효율적이다. Feature별로 학습률을 각기 다르게 조정하는것으로 이를 해결한다.
- 데이터의 특성에 따라 많이 변화된 파라미터는 학습률을 줄이고 적게 변화한 마라미터는 학습률을 높이는 방향으로 학습을 시킨다.

<img width="1256" height="440" alt="image 2" src="https://github.com/user-attachments/assets/bead0987-734d-4e6f-9def-1ec4bd50711c" />


위 식에서 gt가 0인 경우 무한대로 발산할 수 있기 때문에 이를 방지하고지 ε라는 아주 작은 값을 더해준다.

### RMSProp (알엠에스프롭)

- Root Mean Square Propagation'의 약자로, Adagrad의 **'학습 중단 문제'를 해결**하기 위해 제안.
- adagrad는 학습이 진행될때 학습률이 꾸준히 감소하다 나중에는 0에 수렴하는 한계가 존재.

<img width="1072" height="520" alt="image 3" src="https://github.com/user-attachments/assets/a3aa3200-cea7-449d-947a-fad1af1c1b7d" />


### Adam

- 'Adaptive Moment Estimation'의 약자로, **"Momentum + RMSProp"**의 장점을 결합한 옵티마이저
- Momentum의 **방향성(관성)**과 RMSProp의 **적응형 학습률**을 모두 활용하여 더 빠르고 안정적인 수렴을 추구

<img width="1160" height="746" alt="image 4" src="https://github.com/user-attachments/assets/c340401f-f54f-4546-be97-97b11fa7430f" />


### AdamW

- AdamW는 **'Adam with Decoupled Weight Decay'**의 줄임말로, 딥러닝에서 가장 널리 쓰이는 Adam 옵티마이저의 문제점을 개선한 버전

- 기존 Adam의 문제점
1. 가중치 감쇠 효과가 Adam의 적응형 학습률에 의해 왜곡됩니다.
2. 결과적으로 정규화가 의도한 대로 작동하지 않아, 특히 가중치가 큰 경사도를 가질 때 감쇠 효과가 약해지는 등 일반화 성능이 저하될 수 있었습니다.

- AdamW의 개선방안
1. 오직 순수한 손실 함수의 경사도만 계산 ➔ 이 경사도로 Adam 업데이트 (모멘텀, 적응형 학습률 적용)
2. 가중치 업데이트 마지막 단계에서, 이와 별도로 '가중치 감쇠'(2번 방식)를 직접 적용하여 가중치를 감소시킵니다.
